Andrew G Haldane: Towards a common financial language1 
Speech by Mr Andrew G Haldane, Executive Director, Financial Stability, Bank of England, at 
the Securities Industry and Financial Markets Association (SIFMA) “Building a Global Legal 
Entity Identifier Framework” Symposium, New York, 14 March 2012. 
 The views are not necessarily those of the Bank of England or the Financial Policy Committee. We would like to 
thank Alan Ball, Peter Davies, Priya Kothari and Colin Miles for comments and contributions. 
In the first book of the Old Testament, God punishes the builders of the Tower of Babel for 
their  folly.  The  choice  of  punishment  is  revealing.  God  replaces  the  Earth’s  then-common 
language with multiple tongues. The costs of that original sin are still being felt. Today, there 
are nearly 7000 languages spoken globally across fewer than 300 nation states. 
Linguistic  diversity  has  of  course  cultural  benefits.  But  this  comes  at  some  considerable 
economic cost. A common language has been found to increase dramatically bilateral trade 
between  countries,  by more  than  40%.2  It  has  been  found  to  increase  bilateral  investment 
between two countries by a factor of three or more.3 In Iceland at the start of the century as 
much as 3% of annual GDP was, quite literally, lost in translation.4 
Finance today faces a similar dilemma. It, too, has no common language for communicating 
financial  information.  Most  financial  firms  have  competing  in-house  languages,  with 
information  systems  silo-ed  by  business  line.  Across  firms,  it  is  even  less  likely  that 
information systems have a common mother tongue. Today, the number of global financial 
languages very likely exceeds the number of global spoken languages. 
The economic costs of this linguistic diversity were brutally exposed by the financial crisis. 
Very few firms, possibly none, had the information systems necessary to aggregate quickly 
information on exposures and risks.5 This hindered effective consolidated risk management. 
For some of the world’s biggest banks that proved terminal, as unforeseen risks swamped 
undermanned risk systems. 
These  problems  were  even  more  acute  across  firms.  Many  banks  lacked  adequate 
information on the risk of their counterparties, much less their counterparties’ counterparties. 
The  whole  credit  chain  was  immersed  in  fog.  These  information  failures  contributed 
importantly  to  failures  in,  and  seizures  of,  many  of  the  world’s  core  financial  markets, 
including the interbank money and securitisation markets. 
Yet  there  are  grounds  for  optimism.  Spurred  by  technology,  other  industries  have  made 
enormous strides over recent years towards improving their information systems. Improved 
visibility of the network chain has transformed their fortunes. It has allowed the management 
of risks previously unmanageable. And it has allowed the development of business practices 
and products previously unimaginable.  
To  gauge  that,  we  begin  by  discussing  the  progress  made  by  two  well-known  industrial 
networks  over  recent  years  –  product  supply  chains  and  the  world  wide  web.  For  both,  a 
common  language  was  the  prime-mover  behind  a  technological  transformation.  That 
transformation has delivered huge improvements in system resilience and productivity.  
                                                 
1  Paper by Robleh D Ali, Andrew G Haldane and Paul Nahai-Williamson. 
2  Oh et al (2011). 
3  Oh et al (2011). 
4  Ginsburgh and Weber (2011). 
5  Counterparty Risk Management Policy Group (2008). 
BIS central bankers’ speeches 
 1
Compared with these industries, finance has been a laggard. But the tide is turning. Since 
the  crisis,  significant  progress  has  been  made  by  financial  firms,  regulators  and  trade 
associations in developing information systems and the common data standards necessary 
to underpin them. Conferences like this are evidence of that.  
At  the  same  time,  there  is  a  distance  to  travel  before  banking  harvests  the  benefits  other 
industries have already reaped. A common financial language has the potential to transform 
risk  management  at  both  the  individual-firm  and  system-wide  level.  It  has  the  potential  to 
break-down barriers to market entry, leading to a more decentralised, contestable financial 
sector. And it thereby has the potential to both boost financial sector productivity and tackle 
the too-big-to-fail problem at source.  
There is no technological barrier to this transformation. And private and social interests are 
closely aligned as improved information systems are a public as well as a private good. That 
public-private partnership is one reason why such good progress has been made over the 
past few years. So stand back for a good news story – a story of how finance could transform 
both itself and its contribution to wider society.  
Lessons from other industries  
In  look  and  feel,  the  topology  of  the  financial  network  is  part  product,  part  information 
network. The multiple products and counterparties of financial firms have a close parallel in 
the  behaviour  of  non-financial  firms  operating  in  a  product  supply  chain.  And  the  complex 
configuration of multiple criss-crossing nodes and links in the financial web are hallmarks too 
of global information webs. 
Yet despite these similarities, finance lags by a generation both products and information in 
the management of its network. Today’s financial chains mimic product supply chains of the 
1980s  and  the  information  chains  of  the  1990s.  For  global supply  chains  and the  internet, 
their  fortunes  were  transformed  by  a  common  language.  This  enabled  them  to  become 
global in scale and scope and highly adaptive to new demands and technologies. They are 
astonishing success stories. 
Product Supply Chains 
(a) 
On 26 June 1974, Clyde Dawson bought a pack of chewing gum from a Marsh supermarket 
in  Troy  Ohio.  Twenty-five  years  later,  this  event  would  be  celebrated  in  a  Smithsonian 
Institute exhibition. This was the first time a barcode had been scanned. 
Barcodes are the most recognisable element of what, over the past 40 years, has become a 
global  language.  Before  barcodes,  suppliers,  manufacturers  and  retailers  recorded  their 
products in bespoke languages. This was a recipe for inefficiency, as goods were recorded 
and  re-recorded  in  different  tongues.  It  was  also  a  recipe  for  confusion  as  there  was  no 
simple  means  of  understanding  and  communicating  along  the  supply  chain.  The  costs  of 
translation were of Icelandic proportions. 
In  the  early  1970s,  members  of  the  Grocery  Manufacturers  of  America  (GMA)  set  about 
tackling this problem. The result was a Uniform Product Code (UPC), a common standard for 
recording the identity of products. The UPC was launched in 1974, at first for retail products, 
using barcode technology. It immediately took off. By 1977, the UPC had become a global 
standard for retail products. 
During the 1980s and 1990s, this UPC technology extended its reach along the supply chain. 
It did so by developing a system for locating products as well as identifying them. A Global 
Location  Number  (GLN)  was  introduced  in  1980,  tying  products  to  fixed  locations.  And  in 
1989,  the  Serial  Shipping  Container  Code  (SSCC)  was  devised,  enabling  products  to  be 
grouped in transit. 
These common data standards provided, for the first time ever, a vista on the global supply 
chain  –  a  map  of  the  end-to-end  production  conveyor  belt  as  products  moved  between 
2 
 BIS central bankers’ speeches
locations. This transformed production processes. For example, invoicing, purchase orders, 
despatch  advice  and  receipt  advice  could  now  be  standardised  across  firms  in  the  new 
common language.  
Because  the  language  of  individual  countries  was  no  longer  an  obstacle  to  trade,  these 
standards helped to expand existing, and to create new, global supply links. From the 1980s 
onwards, supply chains lengthened and strengthened across geographic boundaries. They 
became  genuinely  global.  Between  1980  and  2005,  the  total  value  of  world  exports 
quintupled.6 A common language was their glue.  
Entering  this  century,  these  technologies  have  continued  to  extend  their  reach.  The 
introduction  of  the  Global  Data  Synchronisation  Network  (GDSN)  in  2004  standardised 
product  descriptions  with  over  1,500  attributes,  such  as  width,  depth  and  height.  Products 
were now recorded in a consistent way globally. At a stroke, a unique “DNA string” had been 
devised for each global product, describing its genetic make-up. 
With an emerging language of product identifiers, locators and attributes, the stage was set 
for an umbrella organisation to maintain and develop these common standards. After 2005, 
this  umbrella  unfolded  in  the  form  of  GS1.  This  is  a  global  not-for-profit  association.  GS1 
currently has over 1.5 million members and maintains information on over 40 million products 
in  its  global  registry.  This  is  the  evolving,  expanding,  adapting  genetic  codebook  for  the 
global supply chain.  
World Wide Web 
(b) 
On October 29 1969, the first-ever message was sent between computer networks at UCLA 
and  Stanford  Universities.  The  message  was  only  five  characters  long  (“login”).  It  was  an 
inauspicious start. The system crashed on the letter “g”. The stage was nonetheless set for 
what has become a technological revolution. 
By the end of 1969, a network of campus computers – an embryonic internet – was up and 
running. Applications began to emerge, allowing this evolving network to expand its reach. 
Take electronic mail. 
In  the  early  1970s,  the  US  Defence  Department’s  Advanced  Research  Projects  Agency 
(ARPA)  developed  network  electronic  mail  capabilities.  In  1979,  Compuserve  opened  up 
electronic  mail  to  a  rising  community  of  personal  computer  users.  In  the  same  year,  two 
graduate  students  at  Duke  University,  Tom  Truscott  and  Jim  Ellis,  developed  Usenet, 
allowing users to post messages to (a precursor of) internet forums.  
By 1983, after more than a decade of evolutionary development, a universal Internet Protocol 
was agreed. This provided infrastructure and common technical standards for this evolving 
communications network. But the languages of the internet remained technical and diffuse. 
That meant it largely remained the domain of the nerds – libraries and universities.  
In  March  1989,  Tim  Berners-Lee,  a  British  software  engineer  working  at  CERN,  the 
Geneva-based  nuclear  research  lab,  wrote  a  memo  to  his  boss  Mike  Sendall.7  It  was  a 
suggestion for improving information management at CERN, replacing the silo-ed approach 
to information storage and search with a more interconnected approach. Sendall described 
the proposal as “vague but exciting”. 
A  year  on,  Berners-Lee  made  his  proposal  less  vague  and  more  exciting.  A  paper  with 
Robert  Cailliau  proposed  the  development  of  a  World  Wide  Web,  with  links  between  and 
within documents creating a web-like information network.8 At the centre of this web was a 
                                                 
6  World Trade Organisation (2011). 
7  Berners-Lee (1989). 
8  Berners-Lee and Cailliau (1990). 
BIS central bankers’ speeches 
 3
common language – Hyper-Text Markup Language (HTML). This was a new lingua franca for 
computers,  allowing  them  to  communicate  irrespective  of  their  operating  systems  and 
underlying technologies. 
To  accompany  this  new  language,  Berners-Lee  created  some  new  co-ordinates  for  this 
global  web  –  Universal  Resource  Identifiers,  the  most  well-known  of  which  is  the  URL 
(Uniform  Resource  Locator).  These  were  a  unique  character  string  for  identifying  web 
locations. With a common language, common locators and an agreed syntax, the stage was 
set for the web to spread (as Berners-Lee had originally envisioned) worldwide. 
At  first  progress  was  slow.  In  June  1993,  there  were  still  only  130  websites.  But  as  web 
browsers became available for home use, the web began to spread exponentially. By 1996, 
over 15 million users were surfing 100,000 websites. Today, 7 million web sites and around 
the  same  number  of  users  are  added  each  week,  with  over  2  billion  web  users  in  almost 
every  country  in  the  world.  A  common  language  has  been  the  linchpin.  Like  all  good 
languages,  HTML  has  had  to  adapt  to  these  changing  demands,  with  Version  5  currently 
under development.  
Since 1994, the evolution and the maintenance of web standards have been overseen by a 
global  organisation  –  the  World  Wide  Web  Consortium  (W3C).  W3C,  like  GS1,  is  a  
not-for-profit organisation, with 346 members drawn from across the globe.9  
Measuring the benefits of a common language 
So taking these two examples, what benefits has a common language been able to deliver?  
Improving risk management in firms 
(a) 
Gobal companies face a hugely complex logistical problem. They manage multiple products, 
in  multiple  locations,  with  multiple  suppliers  operating  to  multiple  time-lines.  Effective 
inventory  management  along  the  supply  chain  is  vital.  Common  data  standards  and 
technology have together transformed this process. 
Global supply chain giants come no larger than Wal-Mart Stores Inc. It employs more than 
2 million people in 28 countries across nearly 10,000 stores. Its total revenues, at over $400 
billion annually, are almost four times higher than its nearest competitor. By comparison, one 
of the world’s largest banks by assets, Citigroup, has revenues of just over $100 billion, with 
just over 250,000 employees.  
Automation  of  supply  chain  management,  facilitated  by  data  standardisation,  has  been 
central  to  Wal-Mart’s  success.  Wal-Mart  was  an  early  adopter  of  common  data  standards 
and technology. By 1998, around 85% of Wal-Mart’s retail products were distributed using 
standardised barcodes and handheld computers. These identified and tracked the location of 
products in real time as they were distributed to individual stores. 
This technology has since been extended to individual stores. Manual barcode scanning of 
shelves and supplies has been largely replaced by contactless tagging of container products 
using  radio  frequency  GS1  identifiers.  In  2010,  Wal-Mart  started  tagging  individual  items, 
further improving inventory-tracking. By combining real-time inventory and point-of-sale data, 
stock-checking  and  shelf-replenishment  has  become  almost  fully-automated.  Thinking 
machine has replaced man throughout the supply chain.  
This  “warehouse-to-shelf”  inventory  management  process  has  transformed  Wal-Mart’s 
business.  Full  visibility  of  the  internal  supply  chain  has  enabled  close  to  real-time  tracking 
and management of stock across many of its stores. It has allowed genuinely consolidated 
inventory risk management. The results have been tangible. The risk of being out-of-stock in 
                                                 
9  As at 6 March 2012. 
4 
 BIS central bankers’ speeches
Wal-Mart stores equipped with radio frequency technology has been found to be 30% lower 
than in stores without that technology.10  
More  resilient  supply  chain  management  has  also  been  a  trademark  of  a  number  of 
web-based  companies.  In  September  1995,  Pierre  Omidyar  created  a  website  for  online 
buying and selling.11 He did the programming over a weekend and launched Auction Web 
using the domain name he had bought for his freelance consultancy Echo Bay Technology 
Group (ebay.com). 
Omidyar put a broken laser pointer for sale. At the end of its first day, the site had attracted 
no  visitors.  Even  as  transactions  grew,  so  did  disputes  between  buyers  and  sellers. 
Transactions  between  anonymous  traders  created  enormous  information  problems.  With 
sellers  knowing  more  than  buyers  about  their  goods,  the  market  risked  being  flooded  by 
“lemons”.12  For  a  time,  Omidyar  appeared  to  have  created  the  first-ever  global  market  for 
lemons.  
Information markets such as these are notoriously fragile. But the web, underpinned by its 
common language, provided eBay  with a solution to its gigantic lemons problem.  Omidyar 
adapted  the  site  to  allow  buyers  and  sellers  to  give  each  other  feedback  on  transactions, 
positive  or  negative.  These  scores  created  a  method  for  buyers  to  assess  risk.  It  also 
incentivised good behaviour. Bad traders were removed from the supply chain, while good 
traders were rewarded with reputational incentives to strengthen their links.  
With  information  problems  mitigated,  eBay  has  gone  from  strength  to  strength.  It  currently 
has  100  million  active  users.13  The  feedback  system  has  remained  eBay’s  core  risk 
assessment  tool.  It  has  since  been  emulated  by  other  web-based  marketplaces,  such  as 
Etsy and Amazon. Each uses the information embedded in the web, conveyed in a common 
language, to squeeze even the largest of lemons.  
Improving risk management across firms 
(b) 
The potential to improve resilience through a common language is, if anything, greater still 
when  managing  risks  across  the  network.  Take  Wal-Mart.  It  is  connected  to  all  of  its 
suppliers through a central inventory management network, giving suppliers full visibility of 
product inventories at Wal-Mart stores. Indeed, Wal-Mart requires that its suppliers use this 
network information to manage the frequency, timing and quantity of their product shipments 
to its stores. This is known as the continuous replenishment process.14  
The  level  of  supply  chain  visibility  in  this  system  has  enabled  suppliers  to  fine-tune  their 
responses to shifts in demand. This reduces the risk of being out of stock. It also reduces 
instabilities  which  otherwise  arise  along 
the  supply  chain  between  manufacturers, 
wholesalers and retailers. For example, it is well-known that small shifts in demand tend to 
get  magnified  as  they  move  along  the  supply  chain.  This  phenomenon  is  known  as  the 
“bull-whip effect”.15  
The  longer  are  supply  chain  lead-times,  and  the  greater  inventory  uncertainties,  the  more 
destabilising  are  these  bull-whip  oscillations.  Wal-Mart’s  continuous  replenishment  process 
has compressed dramatically those lead times and uncertainties. As a result, the crack of the 
                                                 
10  Hardgrave et al (2006). 
11  Cohen (2002). 
12  Akerlof (1970). 
13  eBay (2012). 
14  Grean and Shaw (2002). 
15  Lee et al (1997). 
BIS central bankers’ speeches 
 5
bull-whip  has  been  nullified  along  the  product  supply  chain,  reducing  system-wide 
instabilities. 
For  Wal-Mart,  common  standards  have  revolutionised  ex-ante  risk  management,  allowing 
real-time  aggregation  and  visibility  of  risks  and  opportunities  along  the  supply  chain.16  For 
other industries, a common language has also transformed ex-post risk containment. Take 
the pharmaceuticals industry. 
Building  on  existing  GS1  standards,  individual  packets  of  drugs  can  now  be  assigned  a 
unique serial number by the manufacturer, along with information on the batch from which it 
was  produced  and  other  relevant  attributes.  These  data  can  be  shared  in  a  common  pool 
with retailers and distributors. As at Wal-Mart, the packet’s location is logged at every stage 
of the supply chain from factory to pharmacy. 
This  central  supply  chain  information  system  has  transformed  drug  risk  management  in  a 
crisis. In the past, before drug packets had unique identifiers, defective products would have 
resulted  in  a  mass  recall  of  all  packets  of  a  certain  drug  type.  This  often  caused  massive 
disruption  to  the  manufacturer  and  generated  panic,  bordering  on  hysteria,  among 
consumers. The bullwhip effect was powerful and destabilising. 
Today,  those  instabilities  have  been  calmed  courtesy  of  common  standards  and  improved 
technology. If a product needs to be recalled, the manufacturer updates the common data 
pool with an instruction, alerting the distributor or dispenser at the next point in the supply 
chain.  If  a  recalled  product  has  already  been  sold,  the  points  of  sale  can  be  tracked  and 
those affected can be notified. Panic is no longer systemic; it is quickly localised.  
 Mapping the network 
(c) 
If data are not standardised, mapping complex networks is a high-dimension jigsaw puzzle. 
Understanding the underlying picture is taxing and time-consuming. Capturing the same data 
in  a  standardised  way  means  that  plotting  the  network  becomes  a  much  simpler  game  of 
join-the-dots.  Experience  with  both  product  supply  chains  and  the  World  Wide  Web 
demonstrate the benefits of doing just that.  
In  global  product  space,  the  recent  work  of  Ricardo  Hausmann  and  colleagues  gives  us 
some  tantalising  glimpses  of  the  possibilities.  They  have  recently  constructed  an  “Atlas  of 
Economic  Complexity”.17  This  is  a  visual  representation  of  the  product  categories  that 
contribute  significantly  to  a  country’s  exports.  Those  product  categories  can  in  turn  be 
mapped to measures of complexity. For example, iPhones are the fruits of a more complex 
manufacturing process than ice-cubes.  
This product network mapping provides new insights into trading opportunities and risks. For 
example,  it  allows  country-specific  measures  of  economic  complexity  to  be  constructed. 
These can explain almost three-quarters of the variation in income across countries. A one 
standard deviation rise in a country’s “economic complexity” has been found to increase its 
long-term growth rate by a remarkable 1.6% per year.  
If these numbers are even roughly right, this product atlas is a fantastic navigation device. It 
could  help  guide  countries  towards  investing  in  their  most  profitable  industries.  Because  it 
also shows how closely product types are linked, both within and across countries, product 
and country complexity maps might also be used to help chart a course for industrial policy 
and international trade.  
The web is a higher-dimension network than the global supply chain. Navigating it is more 
complex still. Early in its life, that seemed likely to retard its progress. In the early 1990s, web 
                                                 
16  Chandran (2003). 
17  Hausmann et al (2011). 
6 
 BIS central bankers’ speeches
search engines relied on the content of each page to determine its relevance. As the web 
grew, simple keyword search was being overwhelmed by the web’s size and complexity. By 
the mid-1990s, search engines were lost in navigation.  
In 1996, two Stanford students, Larry Page and Sergey Brin, devised a solution.18 You are all 
now familiar with its name. It was called “BackRub”. Page and Brin saw the links between 
websites, rather than the content within them, as the key to navigating the web. Using that 
basic  intuition,  they  created  a  search  algorithm  which  calculated  a  ranking  of  the  relative 
importance of every web page in the global network. This ranking is known as PageRank. 
By  drawing  explicitly  on  the  information  embedded  in  the  network,  PageRank  produced 
dramatically improved web search capabilities. By now, the graduate project had become a 
company  called  Google.  The  vast,  growing  multi-dimensional  cats-cradle  of  global 
information could now be sliced and diced in fractions of a second. Navigating the web no 
longer required a specialist pilot; it could be mapped by a novice. The World Wide Web had 
been given an instantaneous backrub. 
That  has  yielded  vast  productivity  improvements.  Google  itself  is  today  valued  at  around 
$200  billion,  having  gained  in  value  by  on  average  $284  million  every  week  of  its  life. 
Applications of Google’s network mapping technology grow by the day. For example, Google 
indicators  produced  from  the  analysis  of  the  popularity  of  search  terms  such  as  “estate 
agents” or “motor vehicles” have outperformed some existing indicators of house prices and 
motor sales.19  
And what has allowed the World Wide Web to be Googled in this way? A common language.  
Lowering barriers to entry  
(d) 
Creating a common language also has the potential to transform the industrial organisation 
of an industry. It does so by simultaneously expanding the frontiers of a global network and 
by  lowering  the  barriers  to  entering  it.  This  allows  new  companies,  with  innovative  and 
superior  product  offerings,  to  quickly  erode  the  market  share  of  incumbents.  Higher 
productivity and growth have been the fruits.20 
Global  supply  chains  are  one  example.  Prior  to  GS1  standards,  small  suppliers  faced  an 
uphill struggle connecting to global supply chains. Big suppliers viewed them as unreliable 
given the costs and difficulty of small suppliers executing real-time inventory management. 
Only large firms with big IT budgets could afford the sunk costs of such systems.  
Today, a real-time inventory management system can be built using GS1 standards for a few 
thousand  dollars.  Small  firms  can  connect  to  global  product  webs,  through  nodes such  as 
Wal-Mart’s  central  inventory  system,  in  the  same  way  they  might  connect  to  a  Facebook 
friend. A common language has levelled the playing field between the corner shop and the 
supermarket. 
The  transformative  powers  of  a  common  language  have  been  even  more  striking  in  such 
well-established  industries  as  publishing,  music,  advertising,  news  and  retailing.  For 
decades, large incumbent firms dominated these markets – EMI, Borders, Tower Records. 
Yet  in  less  than  a  decade,  new  entrants  with  new  business  processes  have  completely 
reshaped the industrial landscape. Take publishing. 
Until  2007,  publishing  was  an  industry  essentially  unaltered  since  Gutenberg.  The  printing 
presses, while bigger and faster, would have been recognisable to a fifteenth century printer. 
                                                 
18  Vise and Malseed (2005). 
19  McLaren and Shanbhogue (2011), Choi and Varian (2011). 
20  Jorgenson et al (2010). 
BIS central bankers’ speeches 
 7
Copying  and  distributing  a  book  to  the  public  involved  a  complex  chain  of  printers, 
wholesalers and retailers. This lengthy supply chain added both risk and cost. It also created 
a  significant  barrier  to  entry  for  new  authors  who  were  reliant  on  publishers  to  get  their 
physical books onto physical shelves. 
Digital  publishing  and  distribution  has  fundamentally  changed  this  market.  Both  the  books 
and the shelves are now electronic. Authors can now format books for e-readers in a couple 
of  hours.  Sales  platforms  like  Amazon  Kindle  Direct  give  access  to  a  worldwide  audience 
with  essentially  no  upfront  costs.  Supply  can  close  to  perfectly  match  demand  as  the 
marginal cost of creating and delivering an eBook is effectively zero.  
This digital revolution has caused barriers to entry to plummet while simultaneously raising 
returns to author entry: they now receive as much as 85% of the royalties from an eBook, 
compared to as little as 10% from a physical book. Previously unpublished authors now sell 
millions  of  books.  The  incumbent  middle  men  –  book  publishers,  distributors  and  
retailers  –  have  found  their  lunch  devoured  in  one  sitting.  In  effect,  they  have  been 
disintermediated by technology and (in a delicious irony) a common language.  
This pattern has been replicated in the music industry. For half a century or more, the music 
supply  chain  was  unchanged.  New  acts  were  discovered  by  large  record  labels  who 
distributed physical goods (vinyl, tapes or CDs) to retailers for sale to the public. Over the 
past decade, the costs of copying and distributing music have fallen to close to zero. As in 
publishing, barriers to new artist entry have plummeted. The middle-men – music production 
companies and distributors – have seen their piece of the pie eaten.  
And as a direct result of this industrial transformation, the pie itself appears to have grown. 
Studies  of  the  contributors  to  productivity  growth  since  the  1960s  indicate  that  the  two 
top-performing sectors have been retail and wholesale.21 This is attributed to improvements 
in business processes in these sectors – the “Wal-Mart effect”. It is also attributed to wider 
and faster dissemination of innovation courtesy of new entrants. 
Economist W. Brian Arthur believes this digitised network of automated processes, linked by 
a common language, may have put the world on the cusp of a second industrial revolution.22 
Arthur  describes  a  “second  economy”  – 
the  physical  
economy – and predicts it could become larger than the physical economy within a couple of 
decades.  If  so,  those  estimated  benefits  of  a  common  language  –  in  this  case  HTML  and 
GS1 rather than Hindi and Mandarin – will need revising upwards.  
the  digital  root  system  of 
Common language for finance 
A. 
Since the collapse of Lehman Brothers, the international financial policy community and the 
private  financial  sector  has  begun  laying  the  foundations  of  their  own  second  economy.  A 
sequence of international initiatives, often led by the private sector, has taken us materially 
closer to constructing a global financial map. The co-ordinates of that map are defined by a 
fledgling global financial language. 
This  global  mapping  effort  is  underpinned  by  two,  mutually  supporting,  pillars:  greater 
financial data to help forge links in the financial information chain and common standards for 
recording and aggregating these data. Progress on data collection has been material. The 
Financial Stability Board is overseeing efforts to improve data, including on global banking 
interconnections, shadow banks and OTC transactions data.23 
                                                 
21  Jorgenson et al (2010), Reinsdorf and Yuskavage (2010). 
22  Arthur (2011). 
23  Financial Stability Board / International Monetary Fund (2011). 
8 
 BIS central bankers’ speeches
these 
legislative 
initiatives  were  grown  during 
Yet the full benefits of this deeper and richer array of financial network information will only 
be  realised  if  these  data  can  be  simply  aggregated  within  and  across  firms.  That  calls  for 
common data describers and standards. Recent legislation recognises that. In the US, the 
Dodd-Frank Act created the US Treasury’s Office of Financial Research (OFR), which has as 
a core objective data standardisation.24 In Europe, the new European Markets Infrastructure 
Regulation (EMIR) embeds common data standards across the EU.25  
The  roots  of 
the  September  2009 
G20 summit.26 Out of this, the Financial Stability Board (FSB) initiated a programme which 
culminated  in  a  report  to  G20  Finance  Ministers  and  Central  Bank  Governors  in  October 
2010.  This  set  out  a  vision  of  common  standards  which  would  allow  global  financial  data 
aggregation.27 Like Berners-Lee’s 1989 memo, it was vague but exciting. But in the reports 
that followed, two crucial ingredients were identified: a Legal Entity Identifier (or LEI) and a 
Product Identifier (or PI).28  
Though  these  acronyms  are  new,  the  principles  underpinning  them  are  not.  LEIs  are 
effectively  the  nouns  of  a  new  financial  vocabulary,  naming  the  counterparties  to  each 
financial  transaction.  Meanwhile,  PIs  are  the  adjectives  of  this  new  vocabulary,  describing 
the  elements  of  each  financial  transaction.  Together,  LEI-nouns  and  PI-adjectives  are  key 
building blocks of a new common financial language. 
LEIs and PIs have natural counterparts in the world of product supply chains. For LEIs read 
GLNs – the Global Location Numbers that uniquely define nodes in the global supply chain. 
GLNs are ultra-flexible. They can denote something as macro as a Wal-Mart warehouse or 
as  micro  as  a  specific  shelf  in  a  Wal-Mart  store.  They  provide  the  co-ordinates  of  a 
high-definition map of the global supply chain. 
For  PIs  read  GTINs  (Global  Trade  Identification  Numbers)  and  the  GDSN  (Global  Data 
Synchronisation  Network).  Together,  these  identifiers  define  not  only  products  (such  as 
toothpaste) but their attributes (the dimensions of the box the toothpaste is stored in). They 
too are ultra-flexible. These identifiers define the DNA string for everything from chocolate 
wafers to collateral swaps. 
LEIs and PIs are being cast from a similar mould to GLNs and GTINs. International efforts so 
far have focussed on global LEIs. These are means of capturing, in a common data string, 
the  old  banking  maxim  of  “know  your  counterparty”.  In  today’s  financially  interconnected 
world,  knowing  your  counterparty  may  be  insufficient.  So  global  LEIs  aim  to  barcode 
counterparty linkages at any order of dimensionality. 
In November 2010, the OFR issued a policy statement on LEIs.29 This kicked-off an industry 
process, led by the Securities Industry and Financial Markets Association (SIFMA), aimed at 
settling on a single, industry-wide entity identification standard. This industry process formally 
began in January 2011, issued its requirements document in May 2011 and recommended a 
consortium  led  by  the  Depository  Trust  and  Clearing  Corporation  (DTCC)  as  its  preferred 
provider in July 2011.  
DTCC is currently running a private sector entity identification pilot project, starting with over 
the counter derivatives. Although this market is fairly concentrated, building the database still 
                                                 
24  Dodd-Frank Wall Street Reform and Consumer Protection Act (2010). 
25  European Commission (2010). 
26  G20 (2009). 
27  Financial Stability Board (2010). 
28  CPSS-IOSCO (2011). 
29  Office of Financial Research (2010). 
BIS central bankers’ speeches 
 9
poses challenges. DTCC anticipates the database could contain over 1 million entities. This 
pilot  will  provide  valuable  experience  when  moving  to  a  broader  range  of  products  and 
market participants. 
The FSB is constructing an LEI roadmap, harnessing the efforts of both official and private 
sectors. Its destination is a single, global LEI standard. As announced last week, significant 
progress  is  being  made  across  five  fronts:  governance,  operational  model,  scope  and 
access,  funding  and  implementation  and  phasing.30  The  FSB  will  present  its  LEI 
recommendations to the next G20 summit in June 2012. 
Creating a PI for finance is altogether more complex. Finance is in the pre-barcode era for 
retail products or the pre-web era of the internet. But enough work has already been done to 
demonstrate  that  PIs  are  feasible. As  long  ago  as  1997,  the  International  Organization  for 
Standardization  (ISO)  released  a  standard  for  the  Classification  of  Financial  Instruments 
(ISO 10962) and designated the Association of National Numbering Agencies (ANNA) as the 
registration body for this standard.31 
More  recently,  following  G20  commitments,  the  industry  has  been  working  to  standardise 
classification  of  OTC  derivative  products.  The  International  Swaps  and  Derivatives 
Association (ISDA) is working with derivatives clearing houses and other financial institutions 
to use Financial product Mark-up Language (FpML), which is already used in the industry for 
communications  and  processes,  to  provide  a  standardised  presentation  of  each  listed  or 
cleared product. ISDA published the latest version of its OTC taxonomies in March 2012.32 
SIFMA is backing the use of FpML.  
At a greater level of granularity still, LexiFi is selling a commercial programming language, 
MLFi,  which  breaks  down  financial  transactions  into  their  primitive  constituents.  These 
elements can be combined using a standardised syntax to describe instruments at any level 
of  complexity.  In  essence,  this  approach  is  an  attempt  to  create  a  “DNA  string”  for 
derivatives. 
As yet, we are some distance from having a consistent global method for financial product 
identification. The prize beyond that, a big one, would be to integrate LEIs and PIs using a 
commonly-agreed global syntax - an HTML for finance. This would allow us to move from 
words  to  sentences.  It  would  provide  an  agreed  linguistic  base  for  a  common  financial 
language. Provided this language is flexible enough, like GS1 standards or HTML, it ought to 
be capable of describing any instrument whatever their underlying complexity.  
Once  developed,  maintaining  and  updating  this  common  language  and  syntax  is  likely  to 
need some global governance body to act as guardian. This is akin to the role played by GS1 
and  W3C  for  supply  chains  and  web  standards.  The  success  of  these  global  standards 
organisations, without the need for regulatory intervention, sets an encouraging precedent for 
finance. 
Using the common language of finance  
So if a common language for finance were to arise with these features, what benefits might 
be expected? 
Improving risk management in firms 
(a) 
Like  retail  giants,  global  financial  firms  face  a  hugely  complex  logistical  problem.  Like 
Wal-Mart, they produce multiple products, in multiple locations, with multiple counterparties 
                                                 
30  Financial Stability Board (2012). 
31  International Organization for Standardization (2001). 
32  International Swaps and Derivatives Association (2012). 
10 
 BIS central bankers’ speeches
working  to  multiple  timelines.  The  world’s  largest  banks  have  hundreds  of  thousands  of 
assets, tens of thousands of counterparts and operate to timelines ranging between 50 years 
and 50 milliseconds. Like Wal-Mart, avoiding failure relies on the effective management of 
risks arising from these vast financial inventories and relationships. 
The  financial  crisis  exposed  myriad  weaknesses  in  those  risk  management  practices, 
especially among the larger and more complex financial firms. Several reports, from both the 
official  and  private  sectors,  have  exposed  those  deficiencies.33  Many  of  the  lessons  are 
common.  For  example,  it  is  clear  that  the  governance  of  risk  at  many  financial  firms  was 
defective. Risk committees lacked clout.  
But  underlying  these  high-level  problems  was  a  set  of  lower-level  deficiencies  in data  and 
risk capture. IT infrastructure was often ill-suited to capturing these risks. As new products or 
business lines emerged, new bespoke systems were simply added to the legacy IT estate. A 
Tower  of  Babel  was  erected.  Like  product  codes  of  the  1960s,  or  computer  operating 
systems of the 1970s, banks’ data networks became fragmented and unable to communicate 
effectively with one another. 
A  fragmented  data  infrastructure  has  a  number  of  nasty side-effects. One  is  a  reliance  on 
manual intervention to operate data and risk management systems. Complete automation of 
business  processes  -  second  nature  along  the  product  supply  chain  –  is  a  rare  bird  in 
banking. As well as improving efficiency, automation reduces the possibility of misreporting 
of exposures through human error.  
Piecemeal  data  capture  also  complicates  aggregation  of  risk  exposures.  That  in  turn 
undermine  effective  consolidated  risk  management.  In  its  2010  report,  the  Senior 
Supervisors Group found that some firms could take weeks to aggregate exposures across 
their balance sheet.34 Even the best-run firms took several hours or days. This, too, contrasts 
starkly  with  the  real-time  inventory  management  systems  operating  along  manufacturing 
supply chains, not only among the Wal-Marts but among the corner stores. 
These data and risk management problems were well-highlighted in the official report into the 
failure of Lehman Brothers, a firm whose risk management was felt to be close to the frontier 
of risk management best practices ahead of the crisis.35 For example, the mistaken exclusion 
of  a  significant  commercial  real  estate  exposure  of  $6  billion  meant  there  was  serious 
misreporting to the Lehman’s board during the crucial months prior to its failure. Information 
on a significant exposure was lost in translation. 
The  aggregation  of  these  individual  risk  exposures  across  the  balance  sheet  was  also 
seriously defective. From the middle of 2007, individual risk limits at Lehman were breached 
for  the  fixed  income,  real  estate  and  high  yield  business  lines.  Yet  in  the  process  of 
aggregating  these  exposures,  something  too  was  lost  in  translation:  firm-wide  exposures 
were within risk limits, despite individual components being in breach.  
It  is  clear  that  these  failures  in  data  infrastructure  and  aggregation  were  not  unique  to 
Lehman. They were endemic across the financial industry. Recent reports suggest progress, 
but make clear that almost all firms have a distance to travel. Real-time, consolidated data 
capture  is  still  a  distant  aspiration.  As  long  as  that  is  the  case,  real-time  consolidated  risk 
management remains a pipe dream.  
But  it  is  a  dream  that  has  hope  of  becoming  reality.  Missing  inventories  and  mistaken 
counterparties could be all but eliminated if financial firms’ information systems spoke in a 
                                                 
33  See,  for  example,  Counterparty  Risk  Management  Policy  Group  (CRMPG)  (2008)  and  Senior  Supervisors 
Group (2009). 
34  Senior Supervisors Group (2010). 
35  Jenner and Block (2011). 
BIS central bankers’ speeches 
 11
common  tongue.  That  is  one  of  the  key  messages  of  the  2010  Senior  Supervisors  Group 
report. The experience of global supply chains and the web tell us there is no technological 
constraint on real-time measurement and management of financial inventories by even the 
largest firms. Where Wal-Mart has led, Wall Street could follow.  
Common data standards could similarly transform consolidated risk management. There is 
no technological reason why slicing and dicing of the aggregate balance sheet could not be 
done with the same simplicity as searching the web. Determining aggregate exposures to a 
given  counterparty  or  a  risk  factor  is  a  single  cut  of  the  financial  web.  It  ought  to  be  as 
straightforward as Googling Aung San Suu Kyi or Britney Spears. 
 Improving risk management across firms 
(b) 
Measuring and managing risks across firms was perhaps the single most important cause of 
the seizure of financial markets during the crisis. Counterparty risk reigned supreme. In as 
densely-connected a web as global finance had become, this was hardly surprising. Network 
risk, the risk of cascading dominos, was several orders of magnitude larger than at any time 
in human history. 
To see that, compare the failure of two financial leviathans only a decade apart. When LTCM 
collapsed  in  1998,  it  had  exposures  of  $150  billion  in  OTC  derivatives  with  a  handful  of 
Wall Street’s finest. When Lehman Brothers failed in 2008, it had OTC derivatives exposures 
alone of $800 billion with thousands of counterparties. It should have come as a surprise to 
no-one that the financial bullwhip effect from Lehmans’ failure was far more painful than from 
LTCM. 
Yet  it  did.  That  is  because,  at  the  time  of  Lehman’s  failure,  no  data  existed  to  monitor 
meaningfully  this  network  risk.  Like  the  supermarket  inventories  of  the  early  1970s,  the 
inventory  list  of  bilateral  exposures  among  the  world’s  biggest  banks  was  dog-eared, 
sparsely populated and out of date. Over the past three years, a variety of initiatives have 
sprung up to fill this vacuum.  
One  key  plank  has  been  better  quality  and  more  frequent  data  on  links  between  the  key 
global  financial  nodes  –  a  full  matrix  of  bilateral  exposures,  on  and  off  balance  sheet, 
between  the  world’s  largest  financial  firms.  The  Financial  Stability  Board  is  orchestrating 
efforts  in  this  area.36  In  time,  this  will  become  one  of  the  bedrocks  of  effective  risk 
management  across  the  global  financial  system.  Having  standardised  LEIs  will  make 
constructing that counterparty matrix far simpler. 
A  second  plank  of  reform  has  been  to  put  such  data  to  use  in  helping  insulate  the  global 
financial network against the failure of one of its key nodes. In November 2011, the Financial 
Stability Board announced that in future the world’s most systemically important banks would 
be required to hold additional capital.37 These systemic surcharges are calibrated according 
to,  among  other  things,  very  simple  measures  of  firms’  direct  interconnectivity  with  other 
firms in the global financial network.38  
More complete counterparty data, collected according to common standards, would enable a 
much more accurate calibration of the financial interconnectivity of key financial nodes. For 
example, a number of researchers have already begun to explore using Google’s PageRank 
techniques to determine the systemic risk of financial firms.39 That could in future serve as 
                                                 
36  Financial Stability Board  / International Monetary Fund (2011). 
37  Financial Stability Board (2011b). 
38  Basel Committee on Banking Supervision (2011).  
39  See, for example, Saltoglu and Yenilmez (2011). 
12 
 BIS central bankers’ speeches
the  basis  for  calibrating  systemic  surcharges.  Where  the  information  web  has  led,  the 
financial web could follow.  
A  third  reform  plank  is  the  G20  commitment  to  centrally-clear  standardised  OTC 
derivatives.40 This re-wires the global financial web, transforming it from a dense, complex 
cats-cradle  to  a  simplified  hub-and-spokes  configuration.  The  dimensionality  of  the  web  is 
then  radically  compressed  and  counterparty  risk  management  thereby  simplified.  Network 
risk, at least in principle, is much reduced. 
It  is  easy  to  see  how  common  standards  for  identifying  products  and  counterparts  would 
support this shift to central clearing. If clearing houses are not themselves to become a new 
manifestation of the too-big-to-fail problem, clearing house risk management will need to be 
transformed. For example, the setting of margin to cover risks to the clearing house relies on 
real-time  information  on  aggregate  exposures  to  counterparties  and  products.  Common 
standards for data, LEIs and PIs, make that a more realistic prospect. 
A  fourth  dimension  of  the  reform  agenda  is  to  make  the  resolution  of  the  world’s  largest 
financial  firms  easier  and  speedier.  The  resolution  of  complex  firms  is  rarely  either. 
Reconciling competing claims on Lehman Brothers’ asset pool has already taken insolvency 
practitioners  three  and  a  half  years  and  thousands  of  man  hours.  It  will  take  many  more 
years before it is complete. 
Homogenised  information  systems,  which  can  be  aggregated  across  business  line  and 
counterparty,  could  transform  this  resolution  process.  This  relies  on  effective  and  timely 
information  systems  on  the  key  counterparties  and  the  key  risk  exposures.  Counterparty 
traceability,  like  counterfeit  drug  traceability,  is  central  to  containing  the  panic  once  failure 
occurs. For both, a common language is key.  
Mapping the network 
(c) 
The  maps  of  the  financial  world  being  used  and  developed  by  regulators  today  broadly 
resemble those used and developed by cartographers in the 15th century. Large parts of the 
geographic  globe  were  at  that  point  uncharted  territory,  a  source  of  fear  and  foreboding. 
Today, large parts of the financial globe are similarly uncharted or cast in shadows. They too 
are a source of regulatory fear and foreboding.  
With financial data captured in a homogenous fashion across financial firms, the stage would 
be set for mapping much more comprehensively the contours of the financial world – an atlas 
of financial complexity to mirror Hausmann’s atlas of economic complexity. Technologically, 
there is no reason why tracking the financial web should be any more complex than tracking 
global  supply  chains  or  the  web.  Monitoring  global  flows  of  funds,  as  they  ebb  and  flow, 
should be possible in close to real time. 
To get a sense of the possible, consider the case of meteorology. It operates within a huge, 
interconnected  global  network,  with  cross-border  information  pooling.  The  logistics  are 
eye-watering. Forecasting the weather involves massively large-scale data sampling of over 
100  million  observations  per  day,  across  hundreds  of  thousands  of  locations  scattered 
across the globe, measured along multiple dimensions. 
The result is a very high-dimension information grid, similar in some respects to the web. The 
information embedded in this high-dimension web is then aggregated, filtered and used to 
forecast weather patterns in close to real time. These weather predictions can be telescoped 
to any level of geographic granularity, from the smallest town to the largest continent. And 
the forecasts are made for timelines ranging from minutes to decades ahead.  
                                                 
40  G20 (2008). 
BIS central bankers’ speeches 
 13
There is no reason why, with the right data infrastructure captured in a common language, 
technology could not be used to map and simulate the risk contours of the global financial 
system.  International  regulatory  authorities  would  play  the  role  of  the  meteorologists.  Risk 
warnings,  like  weather  warnings,  would  form  part  of  the  regulatory  toolkit.  So  too  would 
stress-tests of the impact of extreme financial events on the functioning of the global financial 
web.41 
Lowering barriers to entry  
(d) 
Paul Volcker famously commented that the only useful piece of technological innovation in 
banking  over  the  past  30  years  had  been  the  ATM.  What  is  certainly  true  is  that  some 
markets for banking products appear to lack contestability. That has meant large incumbents 
have  often  dominated  the  banking  landscape,  in  much  the  same  way  as  was  true  until 
recently in music and publishing. In banking, this lack of contestability is one root cause of 
the too-big-to-fail problem.  
Consistent  with  that,  birth  and  death  rates  in  banking  are  lower  than  among  non-financial 
companies.  They  are  lower  even  than  in  other  areas  of  finance.  Death  rates  among 
US banks have averaged around 0.2% per year over the past 70 years. Among hedge funds, 
average annual rates of attrition have been closer to 7% per year. Birth rates are similarly 
low. Remarkably, up until 2010 no new major bank had been set up in the UK for a century.  
If product markets and the internet are any guide, a common financial language could help 
lower barriers to market entry, increasing birth rates in banking. They might even begin to 
erode the too-big-to-fail problem through market forces. At the fringes of finance, there may 
already be encouraging signs of network technology reshaping some elements of banking. 
Max Levchin was not too-big-to-fail. Prior to founding Confinity Inc. in 1998 (with Peter Thiel), 
his previous three businesses had all failed. His new business provided a method of secure 
payments between Palm Pilots. By early 2000, however, it was clear that more people were 
using  the  website,  rather  than  handheld  devices,  to  make  payments,  in  particular  for 
purchases across auction websites. Confinity merged to create a new company – PayPal. 
Even as traffic across the site grew, the stage seemed set for Levchin to make it four failures 
in  a  row  as  two  of  the  world’s  biggest  banks  entered  the  market.  Wells  Fargo  and  eBay 
together launched a new venture, Billpoint, for web-based payments. Citibank, too, entered 
the  market  with  c2it.  But,  remarkably,  PayPal  saw  both  off.  The  key  was  superior  fraud 
detection  technology  developed  by  Levchin  using  the  information  embedded  in  the  web. 
eBay acquired PayPal in 2002 for $1.5 billion and closed Billpoint. Today, it processes over 
$300 million in payments globally each working day. 
In  large  parts  of  Africa,  a  different  type  of  payments  technology  has  also  disintermediated 
banks  –  mobile  phones.  In  March  2007,  the  Kenyan  mobile  phone  company  Safaricom 
launched  M-Pesa,  a  service  for  making  payments  between  phones.  Today,  it  has  around 
15 million users reaching over 60% of Kenya’s adult population. The model has spread to a 
number  of  countries  including  Bangladesh,  Uganda  and  Nigeria.  In  response,  incumbent 
banks have been forced to lower significantly their payment costs.  
And  it  is  not  just  payments.  Lending  is  the  new  frontier.  Kiva  is  a  developing  world 
micro-finance organisation with a difference: it is based in San Francisco. Person-to-person 
financing  is  provided  remotely  using  borrower  information  stored  on  Kiva’s  website.  Credit 
assessment  and  tracking  are  done  by  Kiva’s  field  partners.  Each  of  these  is  ranked, 
eBay-style,  using  information  on  their  historical  default  and  delinquency  rates.  This 
disintermediated  market  has  so  far  avoided  both  banana  skins  and  lemons.  Kiva  has  a 
99% repayment rate.  
                                                 
41  Tett (2012), Haldane (2011). 
14 
 BIS central bankers’ speeches
The  model  may  be  set  to  spread  into  advanced  economies.  Commercial  peer-to-peer 
lending,  using  the  web  as  a  conduit,  is  an  emerging  business.  For  example,  in  the  UK 
companies  such  as  Zopa,  Funding  Circle  and  Crowdcube  are  developing  this  model.  At 
present, these companies are tiny. But so, a decade and a half ago, was Google. If eBay can 
solve the lemons problem in the second-hand sales market, it can be done in the market for 
loans. 
With open access to borrower information, held centrally and virtually, there is no reason why 
end-savers and end-investors cannot connect directly. The banking middle men may in time 
become the surplus links in the chain. Where music and publishing have led, finance could 
follow.  An  information  web,  linked  by  a  common  language,  makes  that  disintermediated 
model of finance a more realistic possibility.  
Conclusion 
Steve  Jobs  described  his  approach  to  innovation  as  “trying  to  expose  yourself  to  the  best 
things that humans have done and then trying to bring those things into what you are doing. 
Picasso  had  a  saying:  good  artists  copy,  great  artists  steal.  And  we  have  always  been 
shameless about stealing great ideas”. Language is the original great idea. Finance should 
steal it. 
References 
Akerlof,  G  A  (1970),  “The  Market  for  “Lemons”:  Quality  Uncertainty  and  the  Market 
Mechanism”, Quarterly Journal of Economics, Vol. 84, No. 3, August. 
Arthur, W B (2011), “The Second Economy”, McKinsey & Company 2011. 
Basel Committee on Banking Supervision (2011), “Global systemically important banks: 
assessment  methodology  and  the  additional  loss  absorbency  requirement”,  Bank  for 
International Settlements, November, available at http://www.bis.org/publ/bcbs207.pdf. 
Berners-Lee, T (1989), “Information Management: A Proposal”, CERN, March. 
Berners-Lee, T and Cailliau, R (1990), “WorldWideWeb: Proposal for a HyperText Project”, 
CERN, November, available at http://www.w3.org/Proposal. 
CFTC / SEC (2011), “Joint Study on the Feasibility of Mandating Algorithmic Descriptions for 
Derivatives”, available at www.sec.gov/news/studies/2011/719b-study.pdf. 
Chandran, P M (2003), “Wal-Mart’s supply chain management practices”, ICFAI Center for 
Management Research case collection. 
Choi,  H  and  Varian,  H  (2011),  “Predicting  the  Present  with  Google  Trends”,  available  at 
http://people.ischool.berkeley.edu/~hal/Papers/2011/ptp.pdf. 
Cohen, A (2002), “The Perfect Store: Inside eBay”, Piatkus.  
CPSS-IOSCO  (2011),  “Report  on  OTC  derivatives  data  reporting  and  aggregation 
requirements: consultative report”, available at www.bis.org/publ/cpss96.pdf. 
Counterparty  Risk  Management  Policy  Group  (2008),  “Containing  Systemic  Risk:  The 
Road to Reform”, August, available at http://www.crmpolicygroup.org/docs/CRMPG-III.pdf.  
Dodd-Frank  Wall  Street  Reform  and  Consumer  Protection  Act  (2010),  available  at 
www.sec.gov/about/laws/wallstreetreform-cpa.pdf. 
eBay 
http://investor.ebay.com/releasedetail.cfm?ReleaseID=640656.  
European Commission (2010), “Proposal for a Regulation of the European Parliament and 
of the Council on OTC derivatives, central counterparties and trade repositories”, available at 
http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=COM:2010:0484:FIN:EN:PDF 
Incorporated 
available 
results”, 
(2012), 
2011 
“Full 
year 
at 
BIS central bankers’ speeches 
 15
available 
October, 
available 
November, 
Institutions”, 
Institutions”, 
Information  Gaps: 
Implementation  Progress  Report”, 
Financial Stability Board and International Monetary Fund (2011), “The Financial Crisis 
and 
June,  available  at 
www.imf.org/external/np/g20/pdf/063011.pdf. 
Financial Stability Board (2012), “Technical features of the Legal Entity Identifier”, March, 
available at www.financialstabilityboard.org/publications/r_120307.pdf. 
Financial  Stability  Board  (2011a),  “Key  Attributes  of  Effective  Resolution  Regimes  for 
Financial 
at 
http://www.financialstabilityboard.org/publications/r_111104cc.pdf. 
Financial  Stability  Board  (2011b),  “Policy  Measures  to  Address  Systemically  Important 
Financial 
at 
http://www.financialstabilityboard.org/publications/r_111104bb.pdf. 
Financial Stability Board (2010), “Implementing OTC derivatives market reforms”, available 
at http://www.financialstabilityboard.org/publications/r_101025.pdf. 
G20  Leaders’  Summit  (2009),  “Pittsburgh  summit  declaration”,  September,  available  at 
http://www.g20.org/images/stories/docs/eng/pittsburgh.pdf. 
G20  Leaders’  Summit  (2008),  “Declaration:  Summit  on  financial  markets  and  the  world 
economy”, November, available at http://www.g20.org/images/stories/docs/eng/washington.pdf. 
Ginsburgh, V and Weber, S (2011) “How many languages do we need? The economics of 
linguistic  
diversity”, Princeton University Press.  
Grean,  M  and  Shaw,  M  (2002),  “Supply-Chain  Integration  through  Information  Sharing: 
Channel Partnership between Wal-Mart and Procter and Gamble”, University of Illinois.  
Haldane, A G (2011), “The money forecast”, New Scientist, 10 December 2011. 
Hardgrave, B C, Waller, M, Miller, R and Walton, S M (2006), “RFID’s Impact on Out of 
Stocks: A Sales Velocity Analysis”, University of Arkansas Information Technology Institute, 
January. 
Hausmann, R, Hidalgo, C, Bustos, S, Corsica, M, Chung, S, Jimenez, J, Simoes, A and 
Yildirim, M A (2011), “The Atlas of Economic Complexity: Mapping Paths to Prosperity”, MIT 
Media Lab and Harvard University. 
International  Organization  for  Standardization  (2001),  “Securities  and  related  financial 
instruments – Classification of Financial Instruments (CFI code).” 
International  Swaps  and  Derivatives  Association  (2012),  “OTC  Taxonomies  and  UPI”, 
available at http://www2.isda.org/otc-taxonomies-and-upi/. 
Jenner  and  Block  (2011),  “Lehman  Brothers  Holdings  Inc.  Chapter  11  Proceedings 
Examiner’s Report”. 
Jorgenson, D W, Ho, M and Samuels, J (2010), “New  Data on US Growth by  Industry”, 
World Klems Conference, Harvard University, August.  
Lee,  H  L,  Padmanabhan,  V  and  Whang,  S  (1997),  “Information  Distortion  in  a  Supply 
Chain: The Bull Whip Effect”, Management Science, 43, 1997b, 546–58, April. 
McLaren,  N  and  Shanbhogue,  R  (2011),  “Using  internet  search  data  as  economic 
indicators”, 
at 
http://www.bankofengland.co.uk/publications/Documents/quarterlybulletin/qb110206.pdf. 
Office  of  Financial  Research  (2010),  “Legal  Entity  Identifier  (LEI)  –  Timeline  of  Major 
Events”, 
http://www.treasury.gov/press-center/press-
releases/Documents/081211%20LEI%20Major%20Timeline%20of%20Events.pdf 
Quarterly 
available 
available 
England 
Bulletin, 
Bank 
at 
of 
16 
 BIS central bankers’ speeches
Oh, C H, Selmier, W T and Lien, D (2011) “International trade, foreign direct investment, 
and  transaction  costs  in  language”,  Journal  of  Socio-Economics,  Volume  40,  Issue  6, 
December, pages 732–735. 
Reinsdorf, M and Yuskavage, R (2010), “Exact Industry Contributions to Labor Productivity 
Change”, Chapter 5 in Diewert, W, Balk, B, Fixler D, Fox, K and Nakamura, A, “Price and 
Productivity Measurement: Volume 6, Index Number Theory”, Trafford Press. 
Saltoglu,  B  and  Yenilmez,  T  (2011),  “Analysing  Systemic  Risk  with  Financial  Networks 
During a Financial Crash”, mimeo.  
Senior  Supervisors  Group  (2010),  “Observations  on  Developments  in  Risk  Appetite 
Frameworks and IT Infrastructure”, December.  
Senior  Supervisors  Group  (2009),  “Risk  Management  Lessons  from  the  Global  Banking 
Crisis of 2008”, October. 
SIL 
(2009), 
http://www.ethnologue.com. 
Tett,  G  (2012),  “Guiding  light  needed  to  cut  through  data  fog”,  Financial  Times,  8  March 
2012. 
Vise, D A and Malseed, M (2005), “The Google Story”, Pan Books.  
World  Trade  Organisation 
www.wto.org/statistics. 
“International  Trade  Statistics”,  available  at 
“Ethnologue:  Languages  of 
the  World”,  available  at 
International 
(2011), 
BIS central bankers’ speeches 
 17
