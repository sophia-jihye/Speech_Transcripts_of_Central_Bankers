Hermann Remsperger: Macroeconomic risk and policy responses 
Welcome  address  by  Professor  Hermann  Remsperger,  Member  of  the  Executive  Board  of  the 
Deutsche Bundesbank, at the 7th Bundesbank Spring Conference "Macroeconomic Risk and Policy 
Responses", Berlin, 27 May 2005. 
 I have the pleasure of welcoming you to the 7th Bundesbank Spring Conference which is organised 
jointly by the Humboldt University Berlin, the London-based Centre for Economic Policy Research and 
the Deutsche Bundesbank. The focus of this year’s conference is on “Macroeconomic Risk and Policy 
Responses”. 
Risk and uncertainty are key elements of many decisions, above all those extending into the future. 
Hence, the optimal response to risk has always been a topic of great interest to political, military and 
economic decision-makers. Concern with this crucial issue is also reflected in the works of our great 
philosophers and writers. 
Among  those  men  of  letters,  a  rather  positive  attitude  towards  taking  risks  seems  to  prevail.  For 
example, the Roman poet Ovid claimed that “Both fortune and love befriend the bold”. And Friedrich 
von Schiller wrote “He that is overcautious will accomplish little”.1  
Now, in general, central bankers are not known for their boldness or love of risk, and I would claim that 
there are some good reasons for this. So – as a cautious central banker - I was very happy to find that 
at  least  the  Greek  dramatist  Sophocles  displayed  a  more  balanced  attitude  towards  risk  when  he 
warned that “Quick decisions are unsafe decisions”. 
There is no need to explain to this audience that central banks face many forms of risk when taking 
monetary policy decisions. This was particularly true for the ECB when it started monetary policy in 
January 1999, but it is also true for central banks who act under more normal circumstances. Alan 
Greenspan has  repeatedly  made  the  point  that:  „...uncertainty  is not  just  a  pervasive  feature  of  the 
monetary policy landscape; it is the defining characteristic of that landscape“.2  
Against  this  background,  it  comes  as  no  surprise  that  the  forms  and  consequences  of  risk  and 
uncertainty  for  the  optimal  conduct  of  monetary  policy  have  been  a  very  active  field  of  research  in 
recent years. Let me use this opportunity to render a very brief summary of what we have learned - 
and what we have not learned - from this research so far.  
Today it is generally recognised that maintaining price stability is the best contribution which monetary 
policy can make to long-run macroeconomic welfare. Therefore, most central banks – like the ECB 
and the Bundesbank before it – are committed to the primary objective of achieving and maintaining 
price stability.  
In order to be able to assess the risks to price stability, monetary policy makers need models which 
represent the relevant structural relationships between the price level, the monetary policy instruments 
and other factors affecting price movements like changes in oil prices or exchange rates.  
However, the exact nature of these relationships is surrounded by a high degree of uncertainty. As a 
consequence, the question of which relationships are the “relevant” ones and which simplifications are 
“adequate” is – and will probably always be - a contentious one.  
One of the lessons that we have learned from recent research is that focusing on specification errors 
or  parameter  uncertainty  in  the  neighbourhood  of  a  particular  reference  model  may  dramatically 
understate the true degree of model uncertainty.  
It is therefore advisable for central banks to base their decisions on a broad range of models which 
reflect  various  assumptions  regarding  the  transmission  process.  In  fact,  this  is  the  approach  which 
most central banks – and certainly the ECB – are already practising.  
                                                      
1    “Wer gar zu viel bedenkt, wird wenig leisten” aus: Wilhelm Tell. 
2   Siehe  Greenspan,  Alan:  Monetary  Policy  under  Uncertainty,  Rede  auf  dem  Symposium  der  Federal  Reserve  Bank  of 
Kansas City, Jackson Hole, Wyoming, 29. August 2003, sowie Greenspan, Alan: Risks and Uncertainty in Monetary Policy, 
Rede auf dem Treffen der American Economic Association, San Diego, California, am 3. Januar 2004. 
BIS Review 39/2005 
 1
Now, the question is which weights should policy makers attach to each of these competing models 
when assessing the risks to price stability? This is a very controversial issue. 
Part of the academic literature assumes that policymakers or their staff form priors over the models 
and choose the approach which minimises average expected loss (Bayesian approach).  
In  contrast,  the  robust  control  approach  assumes  that  policymakers  are  completely  agnostic  with 
respect to the probability that a specific model may represent the actual economy and thus choose the 
approach which minimises the maximum loss across all models (minimax preferences).  
In  order  to  implement  the  Bayesian  approach,  policymakers  need  to  be  able  to  attach  a  specific 
probability weight to each of the competing models. While this may be a very difficult task, the minimax 
approach also suffers from several drawbacks.3  
One fundamental problem is that such an approach is vulnerable to policy makers giving excessive 
weight to low probability, but high-cost scenarios. Therefore, as was pointed out by Chris Sims in his 
2001  paper,  this  approach  should  only  be  seen  as  one  method  for  generating  reference  priors.  Its 
results should be compared to more direct approaches to assessing prior beliefs. 
At the end of the day, and here I am happy to quote Mervin King, “there is no escaping the need to 
make judgements about which models are more plausible than others”.4 
Another important lesson we can learn from recent research is that the problem of model uncertainty is 
compounded by measurement problems regarding key variables like real output and its components.  
Moreover,  a  number  of  studies  have  shown  that  the  problem  of  measurement  error  is  likely  to  be 
particularly severe for variables that cannot be observed directly. Take the output gap and the “neutral” 
level  of  real  interest  rates  as  two  variables  which  figure  very  prominently  in  much  of  the  recent 
literature on optimal monetary policy.  
As  a  consequence,  every  attempt  to  fine-tune  the  economy  bears  the  risk  of  policy  errors  with  the 
corresponding  negative  effects  on  price  stability.  In  my  view,  this  line  of  research  has  very  much 
strengthened the case against output stabilisation as a target of monetary policy – a case which was 
already made by Milton Friedman as early as 1961.  
Let me give you another example of a measurement problem which also shows how closely linked 
data and model uncertainty are. In a current research project at the Bundesbank we try to shed some 
light on the factors that drive the recent strong money growth in the euro area.  
Our  study  shows  that  economic  agents  tend  to  increase  their  money  holdings  when  the  financial 
environment is characterised by low yields, high volatility and a change in risk perception. Such an 
environment may be referred to as one of heightened macroeconomic uncertainty.  
Trying  to  integrate  this  uncertainty  into  a  model  for  money  demand  poses  a  severe  measurement 
problem. A variable has to be revealed which cannot be observed directly. We at the Bundesbank try 
to  uncover  such  uncertainties  from  a  variety  of  observable  variables  which  describe  the  financial 
market situation and economic sentiment. The estimated proxy for macroeconomic uncertainty - or risk 
sentiment - helps us to explain recent monetary developments in the euro area.  
Against  this  background,  we  believe  that  difficulties  with  standard  specifications  of  money  demand 
should  not  be  taken  as an  argument  for  abandoning  monetary  indicators  and  monetary  analysis all 
together. Rather, in our view, we need to deepen our understanding of the link between money and 
prices by integrating uncertainty into our analysis of money demand.  
Recent research on data uncertainty has also strengthened the case for a cautious, gradual approach 
to monetary policy as advocated by William Brainard as early as 1967. In particular, this research has 
taught us that monetary policy makers should not react strongly to first releases of data which could 
easily  be  revised.  They  should rather make  a  broadly-based  estimate  of  such  variables,  taking  into 
account information from other, less revision-prone sources as well as the past history of revisions to 
the variable in question.  
                                                      
3   See  Sims,  C.  A.:  The  Pitfalls  of  a  Minimax  Approach  to  Model  Uncertainty,  American  Economic  Review,  Papers  and 
Proceedings, May 2001. 
4   King, M.: Comments on ‘Risk and Uncertainty in Monetary Policy’ by Alan Greenspan, AEA Annual Conference, 2004, p. 3. 
2 
 BIS Review 39/2005
You may have realised that by now, I have already presented a number of arguments which favour 
caution rather than boldness as the adequate approach to monetary policymaking. However, recent 
research has also shown that under certain conditions monetary policy makers may be well advised to 
take the kind of bold decisions advocated by Ovid and Schiller.  
In  this  respect,  the  idea  that  both  the  private  sector  and  the  central  bank  possess  only  limited 
knowledge  about  the  “true”  structure  of  the  economy  is  particularly  relevant.  Therefore  we  have  to 
continually update our estimates for the specific parameters. Moreover, learning by the private sector 
has to be taken into account when taking monetary policy decisions.  
With this kind of learning, a series of inflation shocks can easily turn into a high degree of inflation 
persistence  perceived  by  the  private  sector.  And  this  makes  it  more  difficult  for  the  central  bank  to 
stabilise inflation at the desired low level. 
Few central bankers would dispute the need to firmly anchor long-run inflation expectations at a low 
level. However, there may be different ways to achieve this aim.  
For instance, the ECB is not known for responding aggressively to short-run fluctuations of inflation 
around target. Instead, it relies on a clear definition of its ultimate objective, an explicit medium-term 
orientation, and its two-pillar approach as the primary means to anchor inflation expectations.  
I am very much in favour of the ECB’s approach. At the same time, I do not want to dispute that there 
may be situations when central banks have to act rather aggressively. In particular, this may be the 
case  when  an  economy  is  subject  to  large  negative  shocks  which  could  trigger  off  extremely 
unfavourable developments. I am thinking here of major shocks like a stock-market crash, a financial 
crisis or other extreme events.  
Unfortunately, in such situations, the log-linear approximate models currently available for monetary 
policy  analysis  provide 
idiosyncratic  shocks, 
decision-making at central banks involves significant judgement – judgement which can be drawn, for 
example, from evidence about past behaviour of markets and the economy as a whole. 
While  monetary  policymakers  are  well-advised  to  follow  a  cautious  “steady-as-she-goes”  approach 
under more normal circumstances, boldness may be required in exceptional situations.  
However, this is by no means a case yet closed. Thus I am very interested to see what results come 
out of the studies that will be presented here today and tomorrow. I wish you stimulating discussions 
and a pleasant stay in Berlin. Thank you very much for your attention. 
little  guidance.  Therefore, 
times  of 
large 
in 
BIS Review 39/2005 
 3
