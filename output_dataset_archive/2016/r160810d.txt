   Yannis Stournaras: The use of micro data to support monetary policy 
decisions 
Speech by Mr Yannis Stournaras, Governor of the Bank of Greece, at the Eighth ECB Statistics 
Conference  “Micro  data  for  monetary  policy  decisions:  moving  beyond  and  behind  the 
aggregates”, Frankfurt am Main, 6 July 2016. 
 Thank you for inviting me to speak today on the use of micro data to support monetary policy 
decisions.  A  couple  of  years  ago  the  topic  would  have  looked  surprising,  to  say  the  least. 
Monetary policy is about broad aggregates and the Eurosystem has a clear mandate to keep 
the eurozone’s general price level stable without taking into account the distributional effects 
of its  decisions.  Since micro  data  are all  about the distribution  of income and  wealth, their 
usefulness  for  monetary  policy  is  far  from  obvious.  Indeed  it  may  look  anachronistic.  I 
remember the monetary policies pursued in many countries and in Greece until the 80s, where 
credit policies and selective credit controls were designed so as to channel funds to particular 
economic  sectors  (and  even  sub-sectors)  while  excluding  other  sectors  from  bank  lending 
altogether. The implementation of these credit controls required a significant amount of micro 
data. For example, banks had to report not just total loans to non-financial corporations but 
loans broken down into specific categories like export trade, import trade and tobacco trade. 
The abolition of those controls brought about the abolition of this detailed micro reporting and 
its replacement with the reporting of aggregate balance sheet items. No one is suggesting a 
return  to  the micromanagement  of the  economy.  Central  Banks  should  remain focused  on 
achieving their aim of price stability for the economy as a whole. 
With the advent of the financial crisis, however, we have realized that aggregate data are not 
enough and policy makers need more granular data. This does not mean that there has been 
some sort of a change in the fundamentals of monetary policy objectives. Our focus continues 
to be macroeconomic variables and there is no going back to the detailed redistributive credit 
policies of the past. The use of micro data in today’s monetary policy making serves a rather 
different purpose. Micro data firstly improve our understanding of the transmission mechanism 
of monetary policy and secondly allow us to better understand the aggregate data and thus 
better forecast their evolution. 
These  two  aims  actually  imply  that  micro  data  do  not  only  signify  a  move  beyond  the 
aggregates but also a look behind the aggregates. Micro data can either supplement existing 
aggregate data or replace aggregate data with new data sets of better quality. Micro data can 
enhance the versatility of aggregate data, as they can be adjusted to accommodate financial 
innovation, regulatory changes or behavioral reactions to a changing economic environment. 
Let me start by discussing how granular or disaggregated data can promote the flexibility of 
the  data  used for  aggregate macroeconomic  and  monetary  analysis. The  main  method for 
collecting monetary statistics is what we could name the ‘balance sheet’ method. Statisticians 
ask policy makers to specify their data needs. Once these needs are recorded, statisticians 
prepare templates that look similar to the balance sheets that banks publish. Users have to 
decide on these dimensions. For example, a template on loans can depict economic sectors 
on the columns and maturities on the rows. Users have to decide on the specific maturity bands 
they are interested in, enshrine them in regulations on the collection of statistical data and live 
with this decision for a long period of time. Changing the reporting templates is a difficult and 
costly process not only for central banks but for the industry as well. The Eurosystem is thus 
committed to keeping reporting templates fixed for at least five years. What can we do if we 
have  reasons  to  believe  that  the  instruments  we  include  in  monetary  aggregates  have  to 
change, if the cut-off maturity band for money is not 24 months but 18 months? With aggregate 
data the only thing we can do is have patience, wait for the next round of amendments to 
regulations to take effect, by which time new changes may start becoming necessary. A good 
BIS central bankers’ speeches 
 1 
   example  of this  occurred  in  my  country  in  the  years  after  2000,  when for  various  reasons, 
depositors moved away from ordinary bank deposits and started using repo agreements to an 
increasing degree. Eventually, this issue was addressed and corrected when repos were re-
recognised as deposits, but there still remains an apparent break in the data that complicates 
the analysis and occasionally leads, even careful economists, to erroneous conclusions. 
But with granular data, policy needs can be met immediately. Let me give some examples, 
where I expect the move to micro granular data to make a positive contribution to the quality 
and timeliness of monetary policy analyses and decisions. Let me discuss specific areas where 
micro-data can enrich our analysis and improve the quality of decision making. 
Micro-data can be a way out of Goodhart’s Law. Goodhart’s Law states that as soon as the 
monetary  authorities  start  targeting  a  monetary  aggregate,  this  targeted  aggregate  starts 
misbehaving, as financial innovation leads to the creation of new financial instruments in an 
attempt to circumvent regulatory policies. With micro data, a policy maker should be able to 
follow such developments almost in real time and adjust the targets accordingly. Policy makers 
will have an easy way to check whether the altered behavior of macroeconomic aggregates is 
the result of fundamental economic changes or a form of regulatory arbitrage. 
From  Jensen’s  inequality  we  know  that  if  behavioral  responses  to  monetary  policy  are 
nonlinear, then the economy’s response differs from the average economic agent’s response 
as the variance of agents’ characteristics increases. A more diverse economy (or increased 
inequality) impacts on the transmission mechanism. If the monetary policy instruments’ impact 
on the distribution of agents’ characteristics is small or unpredictable, then we can assume, at 
least as a good approximation, that such distributional changes may add noise but have no 
systematic effect on the behavior of the economy.  
Recently, there has been increasing reason to believe that this may not be the case. There is 
now an expanding body of research on the impact of nonstandard monetary policies on asset 
prices and hence the distribution of wealth. If such a relationship does exist, and if wealth 
impacts nonlinearly on spending and saving activities, nonstandard monetary policies affect 
the  transmission  mechanism.  Having  a  richer  set  of  granular  data  can  help  internalize  the 
impact of monetary policy actions on wealth distribution and ultimately lead to a more precise 
modelling of the transmission mechanism. A much longer line of research looks at the impact 
of wealth or credit constraints on the behavior of households and thus on the transmission 
mechanism. This is important in order to gauge how fast monetary easing will bear fruits by 
fostering demand growth. Micro data can help us go beyond aggregate Euler equations and 
have  a  richer  analysis.  We  can  identify  the  characteristics  of  households  that  are  credit-
constrained, but also how severe the constraints are. We can also examine the importance of 
various forms of wealth (housing or various financial instruments) and how closely households’ 
assets and liabilities are aligned. 
The  Eurosystem  has  long  understood  the  necessity  of  this  kind  of  information  and  has 
organized  the  Household  Finance  and  Consumption  Survey  that  is  conducted  every  2  to 
3 years.  All  euro  area  countries  collect  data  at  the  level  of  the  household  on  income, 
consumption, wealth and debt. The survey has considerably enriched our data on wealth. At 
the  same  time,  answering  the  above  questions  requires  micro  data  –  at  the  level  of  the 
household and/or individual – since this allows for a more precise measurement of the impact 
of monetary policy on the real economy. 
Although we are still in the process of analyzing the results of only the second wave of this 
survey,  we  can  see  that  we  have  the  raw  material  that  can  help  us  answer  a  number  of 
questions. How does wealth feed into consumption behavior? Does the effect of wealth on 
consumption differ across households, depending on their age, whether they are homeowners 
or  not,  whether  they  have  debt?  As  we  move  forward  and  complete  further  waves  of  this 
survey, we shall be able to infer some information on the dynamics of micro-data, and relate 
them to macroeconomic variables. Thus, it is possible to investigate the differential effects of 
asset  price  changes,  consequent  on  a  monetary  policy  decision,  on  individual  euro  area 
2 
 BIS central bankers’ speeches 
   households. Thus rises in equity prices raise inequality, rises in house prices reduce it and the 
impact of changing bond prices is largely neutral. 
The  richness  and  diversity  of  economic  structures  across  the  eurozone  has  added 
complications, but also provided food for thought. An important outcome of conducting this 
survey  simultaneously  for  the  whole  of  the  eurozone  was  that  it  forced  us  to  align  the 
operational definitions of micro variables across many countries, an exercise that will help us 
in the future when we collect other micro-data. It has also provided very concrete examples of 
how  differences  in  economic  structures  have  wider  implications,  like  the  impact  of 
homeownership or population ageing on consumer and saving decisions. 
While  continuing  the  Household  Finance  and  Consumption  Survey  (HFCS),  we  should 
integrate its results with the rest of macroeconomic data. An often unacknowledged secret is 
that in most macroeconomic accounts the household sector is treated as a residual for lack of 
comprehensive information. The HFCS should help us fill this gap and get a better grasp on 
financial relationships in the economy. 
Another  example  of  a  micro  dataset  collected  by  the  ESCB  through  the  Wage  Dynamics 
Network was a survey of firms in each country to determine their wage and pricing policies and 
their responses to shocks to demand. 
The first wave of data was collected before the crisis and sought to investigate wage and labor-
cost dynamics and their relevance for monetary policy. These dynamics have implications for 
how firms and ultimately the real economy in general respond to economic shocks. Thus, firms 
were  asked  how  often  they  change  wages,  how  they  judge  by  how  much  wages  should 
change,  whether  changes  in  wages  then  feed  through  into  price  changes,  the  institutional 
framework of wage bargaining along with the degree of competition in the sector in which they 
operated. The current wave is focusing on the labor market reforms that have occurred in a 
number  of  countries  during  the  crisis  in  an  attempt  to  determine  whether  reforms  are 
associated with a change in wage-setting behavior. It can thus help us understand to what 
extent the crisis has affected microeconomic behavior, in particular by inducing greater price 
and wage flexibility, and to what extent such changes impact on the transmission mechanism. 
As a final example of the growing importance of micro-data, let me remind you of the Governing 
Council’s decision a few weeks ago to proceed with the collection of granular, loan-by-loan, 
credit data, the already famous AnaCredit project. This project, complementing the existing 
Securities Holding Statistics (SHS) database, will allow us to have a quite detailed view of the 
corporate  sector’s  liabilities  and  the  way  they  are  managed.  This  will  allow  a  much  more 
granular analysis of the impact of monetary policies and relate firms’ behavior to a host of 
microeconomic and financial variables, such as size, credit worthiness, sector of the economy 
and so on. We hope to be able to have a more complete and nuanced answer to questions 
that perplex us, such as what is holding back investment at the zero bound, and what is needed 
to kick start the investments. 
Having spent all this time presenting the potential benefits of micro data for monetary policy 
making, I feel an obligation to warn that it will not be plain sailing ahead. We have to work hard 
to resolve many issues that will unavoidably arise.  
First of all, we should deal with concerns that we are creating “Big Brother”. Granular data, 
almost by definition, cause concerns about safeguarding personal information. Most granular 
data of interest to central banks are about corporate entities. They do not contain sensitive 
personal information, but often they contain important market-sensitive information. Even so, 
improper use may be detrimental to some, or they may be used as a coordinating device in an 
oligopolistic setting. To some, such concerns may look like a nuisance that should not become 
an obstacle on the road to a bright new future of “big data”. This would be a mistake. It will only 
raise suspicions and lead to a worsening of the quality of the data. Central banks should be in 
the forefront of developing best practices and adequate safeguards that allow the use of such 
data without impinging on individuals’ privacy. 
BIS central bankers’ speeches 
 3 
   Second, we should be aware of the risk of drowning in a sea of data. As Nobel laureate Herbert 
Simon  put  it:  ‘A  wealth  of  information  creates  a  poverty  of  attention’.  The  rather  obvious 
characteristic  of  micro  data  is  their  size:  micro  data  by  construction  are  an  overwhelming 
amount of information. It is important to look not only at the benefits of micro but also at the 
risks for policy makers. The risk is that policy makers are bombarded with too many numbers 
and, as a result, decision making is delayed. 
A simple example can elaborate this point. Consider one of the key variables monetary policy 
monitors, M3 growth. The aggregate data on M3 is just one number or one time series, say 12 
monthly observations. What are the corresponding micro data? One possible disaggregation 
of the aggregate is to look at the 6,000 numbers describing the evolution of specific types of 
deposits in every single credit institution. What can policy makers do when confronted with, 
say, 6,000 numbers? Not much. Perhaps they will set up a task force to examine the data and 
come back with a report in a month or two. 
It is important to note that policy makers do not and should not really use micro data. Consider, 
for  example,  the  micro  data  collected  through  the  Household  Consumption  and  Finance 
Survey I have mentioned before. Are policy makers supposed to dig into these mass of data? 
Of  course  not.  The  idea  is  to  channel  these  data  into  statistical  models  that  can  produce 
interesting and useful summary statistics and uncover relationships which are important for 
policy success. Policy makers should ask to see a picture of the whole forest, not a list of trees 
and  should  transfer  to  statisticians  and  economists  the  task  of  developing  models  and 
summary statistics that will follow developments and provide answers to their policy concerns. 
In order then not to lose sight of the forest, economists and statisticians must have clever tools 
that  allow  the  proper  manipulation  of  the  micro  data.  Policy  makers  must  demand  useful 
information squeezed out of the micro data so as to gain a better insight on the aggregate data. 
Is M3 growth for real, or just the artifact of regulatory arbitrage? What is driving its growth? Is 
it a widespread phenomenon or a few special outliers? What is behind a surge in credit? A few 
large corporations or many SMEs? 
The point is that statisticians have their work cut out. We do not just ask them to collect the 
biggest amount of data and store them somewhere. We ask them to develop clever, versatile 
tools  that  can  provide  intelligible  answers  to  policymakers’  queries.  Clever  tools  are  thus 
necessary, if policy makers are to profit from granular data. You are all familiar with the famous 
computer principle ‘garbage in, garbage out’. The problem with micro data is not to end up in 
a situation where lots of good quality information goes in but nothing comes out. In order to be 
able  to  manipulate  micro  data  we  need  two  things:  common  identifiers  and  reference 
databanks. These are necessary in order to organize all this huge information in an efficient 
way,  so  that  it  can  be  manipulated  to  produce  the  answers  to  the  questions  asked  by 
policymakers. 
As  I  mentioned  before,  the  extensive  use  of  micro  data  by  central  banks  is  shifting  the 
compilation  burden  from  reporting  institutions  to  NCB  statisticians.  It  is  thus  necessary  to 
provide  to  our  statisticians  adequate  resources  so  that  they  can  cope  with  the  additional 
demands placed on them. 
I can see that we stand at the start of a period where central banks, like everyone else, will 
make use of “big data” and we should learn how to use them to maximize their benefits. While 
these potential benefits are large, the effort needed is equally significant. We need to invest in 
information technology infrastructure, but we also need to educate our statisticians how to deal 
with the new larger and more complicated data sets. The costs will be high and will fall mainly 
on  the  central  banks.  Instead  of  receiving  readily  usable  processed  information,  we  are 
beginning to demand from reporting agents huge amounts of granular information that is then 
processed in-house by our statisticians. There is a need to streamline the process of collecting 
data.  In  particular,  we  should  exploit  to  the  maximum  synergies  between  the  collection  of 
supervisory and (traditionally) statistical data, by developing common definitions to the extent 
possible, or simple rules to transpose the ones into the others. 
4 
 BIS central bankers’ speeches 
   Central banks are leaving the small safe harbor of simple, aggregate data and are opening up 
to the brave new world of granular big data. In order not to get lost, we need new skills, more 
crew, that is statisticians, and stronger vessels, that is better and more versatile models. We 
hope that in the end we shall reach the island of (price) stability.   
BIS central bankers’ speeches 
 5 
