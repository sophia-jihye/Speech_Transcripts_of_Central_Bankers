Jan Smets: New data needs for monetary policy
Speech  by  Mr  Jan  Smets,  Governor  of  the  National  Bank  of  Belgium,  at  the  ninth  European
Central  Bank  Conference  on  Statistics,  Session  1:  New  data  needs  for  monetary  policy,
Frankfurt am Main, 11 July 2018.
 Reflections on the future of monetary policy may require new data needs
I  would  like  to  thank  the  organisers  for  inviting  me  to  this  most  interesting  conference,  and
especially this particular session as it really is about the interactions between statistics and the
future of monetary policy, a hotly debated topic.
Some important (and interrelated) things have been happening over the last decade or so in the
monetary  policy  landscape.  Not  only  have  central  banks  broadened  their  toolkit  to  tackle  the
financial crisis but also the economic world has been faced with structural changes. Besides,
both  the  use  of  new  instruments  and  the  availability  of  new  data  have  spurred  advances  in
monetary policy research.
The  interplay  of  these  phenomena  could  potentially  have  serious  implications  for  the  way  we
think about monetary policy going forward, be it in terms of objectives, instruments, transmission
channels  or  data  monitoring.  But  –  and  that  is  the  message  I  want  to  convey  here  –to  avoid
drawing  overly  hasty  conclusions,  this  requires  careful  reflection  and,  in  some  cases,  implies
new data needs. Only after careful investigation of the issues at stake, can lessons for the future
of monetary policy be drawn.
In  my  remarks  today,  I  will  focus  more  specifically  on  three  phenomena  that  challenge  our
traditional thinking about monetary policy:
First, the role played by heterogeneity – which has been clearly demonstrated by the use of
new  monetary  policy  tools  (typically  more  targeted)  and  something  which  is  increasingly
documented in economic research and data.
Second, digitalisation – probably one of the most notable structural economic changes over
the last few years – which also opens the door to a new world of data.
Third, the changing role of the financial sector.
I intend to raise a number of questions to foster the debate and hopefully help to structure our
thinking.  “How  exactly  have  these  challenges  called  into  question  the  consensus  on  which
monetary policy has been based?  How can new data help to identify whether – and to what
extent – the practice of monetary policy has changed? More speculatively, can new data help
shape a possible "new normal" for monetary policy?” 
Needless to say, I do not want to provide any definite answers to these key questions; they have
far-reaching implications making it unrealistic to settle all these issues on this panel. 
Taking a step back: data and the New-Keynesian consensus 
Before looking ahead, let's first look back a bit.
Data  that  central  banks  traditionally  look  at  are  broadly  tailored  to  the  New-Keynesian  model
paradigm. In this view of the world –  and I am grossly oversimplifying here, not doing justice to
macro-modellers,  nor  to  policy-makers  –    the  central  bank  operates  in  a  framework  where
representative  agents  interact,  production  is  labour-intensive  and  where  the  role  for  financial
factors  is  limited.    Building  on  rational  expectations  and  sticky  prices,  inflation  is  driven  by
 1 / 4
BIS central bankers' speeches
expected  inflation  and  the  anticipated  change  in  real  marginal  costs  –  the  so-called  New-
Keynesian Phillips curve which links economic activity and inflation.
In this set-up, monetary policy should aim for price stability. Doing so requires bringing aggregate
demand into line with the potential output path. The prime way to do so in these models is by
steering individuals’ intertemporal choice between consuming today versus tomorrow. This gives
a  key  role  to  interest  rates,  where  the  working  assumption  is  that  the  central  bank  steers
perfectly the interest rate that is relevant for the representative agent.
The  careful  monitoring  of  macro-economic  aggregates  and  their  projections  successfully
supported monetary policy decisions in this view of the world which seemed fairly appropriate up
to about ten years ago.
Like I hinted at in my introduction, several developments might have called into question this fairly
simple framework. So, I will focus on three key challenges to the standard practice of monetary
policy  and  their  data  dimension  that  I  just  mentioned.  These  are:  heterogeneous  agents,
digitalisation and, finally, the changing role of the financial sector. Let’s look at each in turn.
First challenge: heterogeneous agents
The appropriateness of representative agent models has been challenged quite strongly since
the  crisis.  For  instance,  some  people  have  claimed  that  monetary  policy  tools  aimed  at
stabilising macro aggregates have harmful side effects on specific sectors or types of economic
agents.  The  allegation  that  asset  purchases  increase  wealth  inequality,  that  the  low  rate
environment  “punishes”  savers  or  that  easy  monetary  policy  facilitates  the  survival  of  zombie
firms are just a few examples.
But,  are  we  only  talking  about  possible  side  effects  of  some  measures  here?  I  think  these
reflections are a broader indication of how heterogeneity can also be a transmission channel for
monetary policy. Going one step further, it could appear that monetary policy works more via the
cross-section than via the time dimension which is the traditional New-Keynesian intertemporal
story.  To  put  it  bluntly,  could  it  be  that  an  interest  rate  cut  has  a  bigger  impact  on  aggregate
demand because it shifts income from creditors to debtors – who stand ready to spend –, rather
than via intertemporal substitution?   
Micro heterogeneity and distributional aspects already appear on the monetary policy stage. They
are  backed  up  by  advances  in  theoretical  research. Brunnermeier and Sannikov, for instance,
argue  that  targeted  monetary  policy  leads  to  redistributive  effects  that  help  mitigate  financial
frictions.  I  think  credit-easing  policies  are  an  explicit  example  of  that  since  specific  types  of
lending  are  being  supported.  Newly  developed Heterogeneous Agent  New  Keynesian  models
(HANK  models)  also  help  to  get  essential  insight  on  monetary  policy  transmission  channels
when the assumption of representative agents is abandoned. Such models suggest that forward
guidance  could  be  less  powerful  than  conventional  rate  cuts  because  of  liquidity-constrained
1
households .
This strand of research would benefit from additional data to help rigorously test these theories,
at the euro area level too. For sure, extra data at a fairly granular level, with a panel dimension to
capture effects over time as well, are of interest here. Micro data from the Household Finance
and Consumption Survey (HFCS) are already a step forward and that effort should be continued.
For example, these data have allowed researchers at the ECB to mitigate concerns that the APP
benefits the wealthy at the expense of the poor . Other Eurosystem data initiatives like Anacredit
can also be useful, for instance to study the extent of zombie lending and how it interacts with the
monetary policy stance.
Second challenge: digitalisation 
2
 2 / 4
BIS central bankers' speeches
As we all know, digitalisation of society dramatically changes our lives – how we produce, work,
trade or consume . So what are the consequences for monetary policy?
3
I shall mention two interlinked dimensions here.
First,  digital  products  and  services  raise  issues  with  measuring  the  genuine  level  of  macro
aggregates that central banks typically look at. How to adequately capture quantities when, for
instance, Netflix or Spotify memberships allow unlimited consumption of content? How do we
determine potential output in such economies? And what about measuring consumer prices for
digital service providers such as social network platforms?
Second,  technology  challenges  our  understanding  of  price  dynamics.  Is  price  stickiness  still
relevant  for  digital  transactions?  How  do  prices  behave  when  the  marginal  cost  of  producing
more is very small, even close to zero?
Addressing  all  these  questions  is  no  easy  task.  Overall,  digitalisation  complicates  our
understanding of the transmission process from extra output to inflation. This has implications
not only for the way we model the economy – and here I am thinking about possible adjustments
to the New-Keynesian Phillips curve – but also for the role we devote to monetary policy. Should
monetary  policy  set  different  objectives  if  prices  are  highly  flexible  and  the  costs  of  inefficient
price  dispersion  are  much  smaller  than  presumed?  Too  early  to  tell,  of  course,  but  definitely
worth an in-depth investigation.
Meanwhile, I welcome advances made in measuring macroeconomic aggregates in the digital
economy, in particular consumer prices. Across the Atlantic, the  Billion prices project and Adobe
Analytics data are promising examples of that. They provide tentative evidence that US inflation
could be overestimated, although this result seems to depend on the dataset used. At the euro
area level, national statistical offices’ initiatives on integration of online and scanner prices into
HICP measures, as well as the Eurosystem's choice of investing heavily in research on price-
setting using micro data will certainly help too.
While  digitalisation  challenges  our  thinking  about  macro-economic  accounting,  it  can  also
provide a whole new set of granular and at the same time multidimensional data. In that sense,
Big Data can become our ally. I will briefly come back to this point at the end.
Third challenge: the changing role of the financial sector
A  third  challenge  to  traditional  thinking  relates  to  the  changing  nature  and  role  of  financial
intermediation, well documented in a research area that literally exploded during the last decade.
We have not only witnessed greater fragmentation within the banking sector which has forced us
to  take  unprecedented  non-conventional  measures  to  preserve  a  smooth  transmission  of
monetary policy. We are also observing a slow-moving tendency towards a larger role for non-
banks in the financing of the economy. With the Capital Markets Union, a project I fully endorse,
the role of players outside the traditional banking sector will hopefully get bigger. This justifies
particular vigilance on the part of the ECB to be ready to monitor developments in this area. We
should also make sure we are able to monitor developments in so-called private virtual tokens
that aim to play a role as money – even though I tend to think that these developments are not
(yet)  of  macroeconomic  relevance.  Related  to  this,  the  Fintech  revolution  blurs  the  traditional
boundaries between the financial and the non-financial sector.
When  such  things  are  becoming  more  relevant,  monetary  policy  transmission  can  profoundly
change  and  monitoring  the  traditional  financial  indicators  can  turn  out  to  be  inadequate.
Therefore, good data coverage of new trends in the financial sector is essential. Fortunately, the
Eurosystem plays a proactive role here and I would like to give two examples where new data
play a key role.
 3 / 4
BIS central bankers' speeches
During  the  financial  crisis,  a  Eurosystem-wide  effort  was  launched  to  exploit  bank-level  data
underlying the money and credit aggregates that are monitored in the ECB's monetary analysis.
That  way,  the  Governing  Council  could  assess  in  a  fairly  granular  way  the  transmission  of
measures  via  the  banking  sector.  The  data  also  proved  key  for  calibrating  the  details  of  the
targeted loans we started giving to banks back in 2014.
Thanks to Money Market  Statistical  Reporting  (MMSR),  which  I  recognise  is  a  huge  statistical
challenge, we also have a better view on the workings of euro area money markets. Moreover, it
enables  the  Eurosystem  to  provide  for  a  back-up  risk-free  benchmark  rate  should  currently
available private benchmark rates cease to be published. In this respect, it is very good to see
how  new  economic  realities  are  being  reflected  here:    contrary  to  the  current  benchmarks,
transactions with non-bank money market participants could be included in this new benchmark
too.
Concrete application and conclusion
The three challenges I raised today may not only imply extensive use of existing micro data but
also require further efforts to exploit the new world of data opened up by digitalisation – the so-
called Big Data.
I do not intend to elaborate much on concrete applications and challenges that come with Big
Data. These aspects will certainly be more deeply tackled later today, in the third session of this
conference. That said, I think technology-driven data bring serious challenges from a practical
point  of  view,  above  all  because  their  granularity  is  multi-dimensional. As  correctly  stated  by
Andrew (Andy) Haldane from the Bank of England in a speech he gave earlier this year, it runs
through  their  volume  (cross-section),  their  velocity  (frequency)  and  their  variety.    One  needs
efficient  data  analytics  tools  to  use  the  data  properly  while  being  aware  of  their  limitations  in
terms of privacy and confidentiality.
To  wrap  up,  the  challenge  in  future  will  be  how  to  translate  the  findings  from  new  data  into
concrete  policy  implications. After  all,  the  micro  evidence  has  to  add  up  to  policy  advice  for
monetary policy which is a macro policy with a rather limited set of instruments. Therefore, in
some cases, other policies – such as macro-prudential, fiscal or structural policies – could be
more appropriate for tackling the challenges that new data reveal.
Thank you for your attention.
1
2
3
Liquidity-constrained households are indeed unable or unwilling to borrow against the future rise in income that
the promise of low rates underpins (Kaplan et al.).
 According to Lenza and Slacalek, the effects of QE and unconventional monetary policy on income via lower
unemployment (benefiting the poor) are more significant than the effects of high prices for financial assets
(benefiting the wealthy). 
Globalisation is another structural phenomenon that has the potential to influence our thinking about monetary
policy – a view the BIS often emphasises – and which could also have implications for what data to monitor. As
Claudio Borio from the BIS suggests, many of the issues surrounding digitalisation could apply in a somewhat
similar way for globalisation as well (I refer to his “Through the looking glass"  speech from 2017). 
 4 / 4
BIS central bankers' speeches
